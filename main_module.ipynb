{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as some of the xml files contains -pc in the beginning while corresponding images does not contain -pc in its name\n",
    "#so here the -pc is removed and stored in \"files\" while the name of xml files are stored in \"annots\"\n",
    "path = \"../desktop/objdet/XML\"\n",
    "files=[]\n",
    "annots=[]\n",
    "for i in os.listdir(path):\n",
    "    annots.append(i)\n",
    "    if(i[0]==\"p\"):\n",
    "        i=i[3:]\n",
    "    i=i[:-4]\n",
    "    files.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function traverses through the xml file and then find the min and max, x and y coordinates\n",
    "#It also returns the labels of the object along with the id\n",
    "def xml_parser(address,width,height):\n",
    "    dic = {'None': 0, 'bar': 1, 'caption': 2, 'credit': 3, 'drop-capital': 4, 'floating': 5, 'footer': 6, 'header': 7, 'heading': 8, 'line': 9, 'logo': 10, 'other': 11, 'page-number': 12, 'paragraph': 13, 'pie': 14, 'punch-hole': 15, 'signature': 16}\n",
    "    path = \"../desktop/objdet/XML\"\n",
    "    tree = ElementTree.parse(path+\"/\"+address)\n",
    "    root = tree.getroot()\n",
    "    ids=[]\n",
    "    types=[]\n",
    "    points=[]\n",
    "    xandy=[]\n",
    "    #img_width = width\n",
    "    #img_height = height\n",
    "    for i in root[1]: \n",
    "        for j in i:\n",
    "            ids.append(i.attrib['id'])\n",
    "            try:\n",
    "                types.append(dic[i.attrib['type']])\n",
    "            except:\n",
    "                types.append(0)\n",
    "            l=0\n",
    "            for k in j:\n",
    "                if(l==0):\n",
    "                    x_min = int(k.attrib['x'])\n",
    "                    x_max = int(k.attrib['x'])\n",
    "                    y_min = int(k.attrib['y'])\n",
    "                    y_max = int(k.attrib['y'])\n",
    "                    l=l+1\n",
    "                elif(l<4):\n",
    "                    if(x_min>int(k.attrib['x'])):\n",
    "                        x_min=int(k.attrib['x'])\n",
    "                    if(x_max<int(k.attrib['x'])):\n",
    "                        x_max=int(k.attrib['x'])\n",
    "                    if(y_min>int(k.attrib['y'])):\n",
    "                        y_min=int(k.attrib['y'])\n",
    "                    if(y_max>int(k.attrib['y'])):\n",
    "                        y_max=int(k.attrib['y'])\n",
    "                    l+=1\n",
    "            \n",
    "            #xandy = [x_min/img_width,y_min/img_height,x_max/img_width,y_max/img_height]\n",
    "            xandy = [x_min,y_min,x_max,y_max]\n",
    "            points.append(xandy)\n",
    "    return(points,ids,types)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots = \"../desktop/objdet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PascalVOCDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, roots, splits, keep_difficult=False):\n",
    "        \"\"\"\n",
    "        :param data_folder: folder where data files are stored\n",
    "        :param split: split, one of 'TRAIN' or 'TEST'\n",
    "        :param keep_difficult: keep or discard objects that are considered difficult to detect?\n",
    "        \"\"\"\n",
    "        self.splits = splits.upper()\n",
    "\n",
    "        assert self.splits in {'TRAIN', 'TEST'}\n",
    "\n",
    "        self.roots = roots\n",
    "        self.keep_difficult = keep_difficult\n",
    "\n",
    "        # Read data files\n",
    "        self.images = self.roots+\"/Images/\"\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Read image\n",
    "        image = Image.open(self.images+files[i]+\".tif\", mode='r')\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        # Read objects in this image (bounding boxes, labels, difficulties)\n",
    "        boxes,ids,types = xml_parser(annots[i],image.size[0],image.size[1])\n",
    "        boxes = torch.FloatTensor(boxes)  # (n_objects, 4)\n",
    "        labels = torch.LongTensor(types)  # (n_objects)\n",
    "        difficulties = torch.ByteTensor(labels.shape)\n",
    "\n",
    "        # Discard difficult objects, if desired\n",
    "\n",
    "        # Apply transformations\n",
    "        image, boxes, labels = transform(image, boxes, labels, splits=self.splits)\n",
    "\n",
    "        return image, boxes, labels, difficulties\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n",
    "        This describes how to combine these tensors of different sizes. We use lists.\n",
    "        Note: this need not be defined in this Class, can be standalone.\n",
    "        :param batch: an iterable of N sets from __getitem__()\n",
    "        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
    "        \"\"\"\n",
    "\n",
    "        images = list()\n",
    "        boxes = list()\n",
    "        labels = list()\n",
    "        difficulties = list()\n",
    "\n",
    "        for b in batch:\n",
    "            images.append(b[0])\n",
    "            boxes.append(b[1])\n",
    "            labels.append(b[2])\n",
    "            difficulties.append(b[3])\n",
    "\n",
    "        images = torch.stack(images, dim=0)\n",
    "\n",
    "        return images, boxes, labels, difficulties  # tensor (N, 3, 300, 300), 3 lists of N tensors each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded base model.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/4]\tBatch Time 69.472 (69.472)\tData Time 24.347 (24.347)\tLoss 23.9654 (23.9654)\t\n",
      "[0/4]\tBatch Time 10.561 (10.561)\tLoss 15.1591 (15.1591)\t\n",
      "\n",
      " * LOSS - nan\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss val' at x-index 0\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/4]\tBatch Time 75.703 (75.703)\tData Time 15.056 (15.056)\tLoss 13.6088 (13.6088)\t\n",
      "[0/4]\tBatch Time 10.846 (10.846)\tLoss 11.5557 (11.5557)\t\n",
      "\n",
      " * LOSS - 11.564\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss train' at x-index 1\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/4]\tBatch Time 70.427 (70.427)\tData Time 42.036 (42.036)\tLoss 11.5415 (11.5415)\t\n",
      "[0/4]\tBatch Time 10.758 (10.758)\tLoss 11.7020 (11.7020)\t\n",
      "\n",
      " * LOSS - nan\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss val' at x-index 2\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/4]\tBatch Time 85.431 (85.431)\tData Time 55.553 (55.553)\tLoss 11.5461 (11.5461)\t\n",
      "[0/4]\tBatch Time 10.342 (10.342)\tLoss 11.3696 (11.3696)\t\n",
      "\n",
      " * LOSS - nan\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss train' at x-index 3\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n",
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss val' at x-index 3\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/4]\tBatch Time 76.349 (76.349)\tData Time 24.973 (24.973)\tLoss 11.3816 (11.3816)\t\n",
      "[0/4]\tBatch Time 12.356 (12.356)\tLoss 11.3335 (11.3335)\t\n",
      "\n",
      " * LOSS - 11.320\n",
      "\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n",
      "Epoch: [5][0/4]\tBatch Time 69.488 (69.488)\tData Time 38.192 (38.192)\tLoss 11.3271 (11.3271)\t\n",
      "[0/4]\tBatch Time 10.504 (10.504)\tLoss 11.2748 (11.2748)\t\n",
      "\n",
      " * LOSS - 11.280\n",
      "\n",
      "Epoch: [6][0/4]\tBatch Time 73.454 (73.454)\tData Time 39.860 (39.860)\tLoss 11.3048 (11.3048)\t\n",
      "[0/4]\tBatch Time 10.958 (10.958)\tLoss 11.2320 (11.2320)\t\n",
      "\n",
      " * LOSS - 11.229\n",
      "\n",
      "Epoch: [7][0/4]\tBatch Time 79.031 (79.031)\tData Time 44.315 (44.315)\tLoss 11.2339 (11.2339)\t\n",
      "[0/4]\tBatch Time 10.524 (10.524)\tLoss 11.1540 (11.1540)\t\n",
      "\n",
      " * LOSS - 11.166\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss train' at x-index 7\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/4]\tBatch Time 52.982 (52.982)\tData Time 24.852 (24.852)\tLoss 11.1543 (11.1543)\t\n",
      "[0/4]\tBatch Time 10.264 (10.264)\tLoss 11.1610 (11.1610)\t\n",
      "\n",
      " * LOSS - 11.138\n",
      "\n",
      "Epoch: [9][0/4]\tBatch Time 67.591 (67.591)\tData Time 41.745 (41.745)\tLoss 11.1453 (11.1453)\t\n",
      "[0/4]\tBatch Time 10.314 (10.314)\tLoss 11.0653 (11.0653)\t\n",
      "\n",
      " * LOSS - 11.069\n",
      "\n",
      "Epoch: [10][0/4]\tBatch Time 98.031 (98.031)\tData Time 48.219 (48.219)\tLoss 11.0726 (11.0726)\t\n",
      "[0/4]\tBatch Time 10.439 (10.439)\tLoss 11.0757 (11.0757)\t\n",
      "\n",
      " * LOSS - 11.020\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss train' at x-index 10\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/4]\tBatch Time 89.911 (89.911)\tData Time 53.587 (53.587)\tLoss 11.0369 (11.0369)\t\n",
      "[0/4]\tBatch Time 10.176 (10.176)\tLoss 11.0148 (11.0148)\t\n",
      "\n",
      " * LOSS - 10.962\n",
      "\n",
      "Epoch: [12][0/4]\tBatch Time 97.517 (97.517)\tData Time 66.637 (66.637)\tLoss 10.9602 (10.9602)\t\n",
      "[0/4]\tBatch Time 10.420 (10.420)\tLoss 10.9172 (10.9172)\t\n",
      "\n",
      " * LOSS - nan\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss train' at x-index 12\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n",
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss val' at x-index 12\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/4]\tBatch Time 48.571 (48.571)\tData Time 21.869 (21.869)\tLoss 10.9418 (10.9418)\t\n",
      "[0/4]\tBatch Time 10.472 (10.472)\tLoss 10.8539 (10.8539)\t\n",
      "\n",
      " * LOSS - 10.802\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [14][0/4]\tBatch Time 58.554 (58.554)\tData Time 29.311 (29.311)\tLoss 10.8600 (10.8600)\t\n",
      "[0/4]\tBatch Time 10.328 (10.328)\tLoss 10.6995 (10.6995)\t\n",
      "\n",
      " * LOSS - nan\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss val' at x-index 14\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/4]\tBatch Time 69.149 (69.149)\tData Time 30.292 (30.292)\tLoss 10.7593 (10.7593)\t\n",
      "[0/4]\tBatch Time 10.430 (10.430)\tLoss 10.5712 (10.5712)\t\n",
      "\n",
      " * LOSS - 10.623\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n",
      "Epoch: [16][0/4]\tBatch Time 69.722 (69.722)\tData Time 41.565 (41.565)\tLoss 10.6763 (10.6763)\t\n",
      "[0/4]\tBatch Time 11.062 (11.062)\tLoss 10.4534 (10.4534)\t\n",
      "\n",
      " * LOSS - nan\n",
      "\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss val' at x-index 16\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/4]\tBatch Time 99.867 (99.867)\tData Time 68.499 (68.499)\tLoss 10.4779 (10.4779)\t\n",
      "[0/4]\tBatch Time 10.551 (10.551)\tLoss 9.5447 (9.5447)\t\n",
      "\n",
      " * LOSS - 9.680\n",
      "\n",
      "\n",
      "Epochs since last improvement: 6\n",
      "\n",
      "Epoch: [18][0/4]\tBatch Time 87.650 (87.650)\tData Time 51.823 (51.823)\tLoss 9.7840 (9.7840)\t\n",
      "[0/4]\tBatch Time 10.507 (10.507)\tLoss 13.4681 (13.4681)\t\n",
      "\n",
      " * LOSS - 13.783\n",
      "\n",
      "\n",
      "Epochs since last improvement: 7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss train' at x-index 18\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/4]\tBatch Time 83.176 (83.176)\tData Time 52.811 (52.811)\tLoss 10.1180 (10.1180)\t\n",
      "[0/4]\tBatch Time 10.315 (10.315)\tLoss 8.8659 (8.8659)\t\n",
      "\n",
      " * LOSS - 8.794\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss train' at x-index 19\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/4]\tBatch Time 65.098 (65.098)\tData Time 39.126 (39.126)\tLoss 9.4405 (9.4405)\t\n",
      "[0/4]\tBatch Time 10.328 (10.328)\tLoss 14.4018 (14.4018)\t\n",
      "\n",
      " * LOSS - 14.908\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [21][0/4]\tBatch Time 82.340 (82.340)\tData Time 50.164 (50.164)\tLoss 6.9030 (6.9030)\t\n",
      "[0/4]\tBatch Time 10.325 (10.325)\tLoss 6.9201 (6.9201)\t\n",
      "\n",
      " * LOSS - 6.913\n",
      "\n",
      "Epoch: [22][0/4]\tBatch Time 72.935 (72.935)\tData Time 33.753 (33.753)\tLoss 7.5668 (7.5668)\t\n",
      "[0/4]\tBatch Time 10.452 (10.452)\tLoss 33.1708 (33.1708)\t\n",
      "\n",
      " * LOSS - nan\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss val' at x-index 22\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/4]\tBatch Time 65.512 (65.512)\tData Time 39.718 (39.718)\tLoss 42.5020 (42.5020)\t\n",
      "[0/4]\tBatch Time 10.410 (10.410)\tLoss 10.3590 (10.3590)\t\n",
      "\n",
      " * LOSS - 10.410\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss train' at x-index 23\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/4]\tBatch Time 51.309 (51.309)\tData Time 22.779 (22.779)\tLoss 10.4820 (10.4820)\t\n",
      "[0/4]\tBatch Time 10.423 (10.423)\tLoss 10.3349 (10.3349)\t\n",
      "\n",
      " * LOSS - 10.420\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [25][0/4]\tBatch Time 57.674 (57.674)\tData Time 25.842 (25.842)\tLoss 10.4952 (10.4952)\t\n",
      "[0/4]\tBatch Time 10.405 (10.405)\tLoss 10.2708 (10.2708)\t\n",
      "\n",
      " * LOSS - nan\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss train' at x-index 25\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n",
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss val' at x-index 25\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/4]\tBatch Time 74.394 (74.394)\tData Time 46.024 (46.024)\tLoss 10.3057 (10.3057)\t\n",
      "[0/4]\tBatch Time 10.295 (10.295)\tLoss 10.1341 (10.1341)\t\n",
      "\n",
      " * LOSS - 10.147\n",
      "\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss train' at x-index 26\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/4]\tBatch Time 54.483 (54.483)\tData Time 27.324 (27.324)\tLoss 10.1893 (10.1893)\t\n",
      "[0/4]\tBatch Time 10.271 (10.271)\tLoss 9.9525 (9.9525)\t\n",
      "\n",
      " * LOSS - 9.993\n",
      "\n",
      "Epoch: [28][0/4]\tBatch Time 72.891 (72.891)\tData Time 39.292 (39.292)\tLoss 9.9430 (9.9430)\t\n",
      "[0/4]\tBatch Time 10.712 (10.712)\tLoss 9.9281 (9.9281)\t\n",
      "\n",
      " * LOSS - 9.880\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss train' at x-index 28\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/4]\tBatch Time 86.606 (86.606)\tData Time 49.056 (49.056)\tLoss 9.7556 (9.7556)\t\n",
      "[0/4]\tBatch Time 12.819 (12.819)\tLoss 9.7700 (9.7700)\t\n",
      "\n",
      " * LOSS - 9.742\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss train' at x-index 29\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/4]\tBatch Time 82.756 (82.756)\tData Time 31.227 (31.227)\tLoss 9.7352 (9.7352)\t\n",
      "[0/4]\tBatch Time 10.814 (10.814)\tLoss 9.5679 (9.5679)\t\n",
      "\n",
      " * LOSS - 9.577\n",
      "\n",
      "Epoch: [31][0/4]\tBatch Time 62.871 (62.871)\tData Time 30.206 (30.206)\tLoss 9.5490 (9.5490)\t\n",
      "[0/4]\tBatch Time 10.329 (10.329)\tLoss 9.3386 (9.3386)\t\n",
      "\n",
      " * LOSS - nan\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss train' at x-index 31\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n",
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss val' at x-index 31\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/4]\tBatch Time 71.255 (71.255)\tData Time 44.007 (44.007)\tLoss 9.3720 (9.3720)\t\n",
      "[0/4]\tBatch Time 10.273 (10.273)\tLoss 9.1171 (9.1171)\t\n",
      "\n",
      " * LOSS - 9.132\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [33][0/4]\tBatch Time 65.391 (65.391)\tData Time 25.036 (25.036)\tLoss 9.1257 (9.1257)\t\n",
      "[0/4]\tBatch Time 10.415 (10.415)\tLoss 8.8594 (8.8594)\t\n",
      "\n",
      " * LOSS - 8.832\n",
      "\n",
      "Epoch: [34][0/4]\tBatch Time 71.526 (71.526)\tData Time 45.696 (45.696)\tLoss 8.8388 (8.8388)\t\n",
      "[0/4]\tBatch Time 10.673 (10.673)\tLoss 8.1856 (8.1856)\t\n",
      "\n",
      " * LOSS - nan\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss val' at x-index 34\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/4]\tBatch Time 88.590 (88.590)\tData Time 57.935 (57.935)\tLoss 8.3077 (8.3077)\t\n",
      "[0/4]\tBatch Time 10.292 (10.292)\tLoss 5.7504 (5.7504)\t\n",
      "\n",
      " * LOSS - 5.456\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss train' at x-index 35\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/4]\tBatch Time 85.830 (85.830)\tData Time 47.877 (47.877)\tLoss 6.5374 (6.5374)\t\n",
      "[0/4]\tBatch Time 10.673 (10.673)\tLoss 12.7725 (12.7725)\t\n",
      "\n",
      " * LOSS - 9.004\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [37][0/4]\tBatch Time 62.375 (62.375)\tData Time 28.920 (28.920)\tLoss 6.7666 (6.7666)\t\n",
      "[0/4]\tBatch Time 10.593 (10.593)\tLoss 3.1518 (3.1518)\t\n",
      "\n",
      " * LOSS - nan\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss val' at x-index 37\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/4]\tBatch Time 101.367 (101.367)\tData Time 70.630 (70.630)\tLoss 3.9010 (3.9010)\t\n",
      "[0/4]\tBatch Time 10.259 (10.259)\tLoss 45.0406 (45.0406)\t\n",
      "\n",
      " * LOSS - 28.879\n",
      "\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahakkothari/Desktop/laplotter.py:56: UserWarning: Got NaN for value 'loss train' at x-index 38\n",
      "  warnings.warn(\"Got NaN for value '%s' at x-index %d\" % (label, x_index))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/4]\tBatch Time 77.048 (77.048)\tData Time 47.260 (47.260)\tLoss 21.2728 (21.2728)\t\n",
      "[0/4]\tBatch Time 10.575 (10.575)\tLoss 8.7255 (8.7255)\t\n",
      "\n",
      " * LOSS - 8.751\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'odict_keys' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4e18b997cf6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-4e18b997cf6b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_since_improvement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_best\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mplotter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mredraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0mplotter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/laplotter.py\u001b[0m in \u001b[0;36mredraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_redraw_main_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_redraw_averages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_redraw_regressions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;31m# Add legends (below both chart)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/laplotter.py\u001b[0m in \u001b[0;36m_redraw_regressions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;31m# for loss chart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             lt_regression = self._calc_regression(self.values_loss_train.keys(),\n\u001b[0;32m--> 490\u001b[0;31m                                                   self.values_loss_train.values())\n\u001b[0m\u001b[1;32m    491\u001b[0m             lv_regression = self._calc_regression(self.values_loss_val.keys(),\n\u001b[1;32m    492\u001b[0m                                                   self.values_loss_val.values())\n",
      "\u001b[0;32m~/Desktop/laplotter.py\u001b[0m in \u001b[0;36m_calc_regression\u001b[0;34m(self, x_values, y_values)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m# was indeed that highest x-value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# This could be avoided by tracking the max value for each line.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mlast_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mnb_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'odict_keys' object does not support indexing"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "from model import SSD300, MultiBoxLoss\n",
    "from utils import *\n",
    "from laplotter import LossAccPlotter\n",
    "\n",
    "# Data parameters\n",
    "data_folder = roots  # folder with data files\n",
    "keep_difficult = False  # use objects considered difficult to detect?\n",
    "\n",
    "# Model parameters\n",
    "# Not too many here since the SSD300 has a very specific structure\n",
    "n_classes = 17  # number of different types of objects\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Learning parameters\n",
    "checkpoint = None  # path to model checkpoint, None if none\n",
    "batch_size = 8  # batch size\n",
    "start_epoch = 0  # start at this epoch\n",
    "epochs = 40  # number of epochs to run without early-stopping\n",
    "epochs_since_improvement = 0  # number of epochs since there was an improvement in the validation metric\n",
    "best_loss = 100.  # assume a high loss at first\n",
    "workers = 4  # number of workers for loading data in the DataLoader\n",
    "print_freq = 200  # print training or validation status every __ batches\n",
    "lr = 1e-3  # learning rate\n",
    "momentum = 0.9  # momentum\n",
    "weight_decay = 5e-4  # weight decay\n",
    "grad_clip = None  # clip if gradients are exploding, which may happen at larger batch sizes (sometimes at 32) - you will recognize it by a sorting error in the MuliBox loss calculation\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Training and validation.\n",
    "    \"\"\"\n",
    "    global epochs_since_improvement, start_epoch, label_map, best_loss, epoch, checkpoint\n",
    "\n",
    "    # Initialize model or load checkpoint\n",
    "    if checkpoint is None:\n",
    "        model = SSD300(n_classes=n_classes)\n",
    "        # Initialize the optimizer, with twice the default learning rate for biases, as in the original Caffe repo\n",
    "        biases = list()\n",
    "        not_biases = list()\n",
    "        for param_name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if param_name.endswith('.bias'):\n",
    "                    biases.append(param)\n",
    "                else:\n",
    "                    not_biases.append(param)\n",
    "        optimizer = torch.optim.SGD(params=[{'params': biases, 'lr': 2 * lr}, {'params': not_biases}],\n",
    "                                    lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
    "        best_loss = checkpoint['best_loss']\n",
    "        print('\\nLoaded checkpoint from epoch %d. Best loss so far is %.3f.\\n' % (start_epoch, best_loss))\n",
    "        model = checkpoint['model']\n",
    "        optimizer = checkpoint['optimizer']\n",
    "\n",
    "    # Move to default device\n",
    "    model = model.to(device)\n",
    "    criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy).to(device)\n",
    "\n",
    "    # Custom dataloaders\n",
    "    train_dataset = PascalVOCDataset(data_folder,\n",
    "                                     splits='train',\n",
    "                                     keep_difficult=keep_difficult)\n",
    "    val_dataset = PascalVOCDataset(data_folder,\n",
    "                                   splits='test',\n",
    "                                   keep_difficult=keep_difficult)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                               collate_fn=train_dataset.collate_fn, num_workers=workers,\n",
    "                                               pin_memory=True)  # note that we're passing the collate function here\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                             collate_fn=val_dataset.collate_fn, num_workers=workers,\n",
    "                                             pin_memory=True)\n",
    "    # Epochs\n",
    "    running_loss = 0.0\n",
    "    plotter = LossAccPlotter()\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        # Paper describes decaying the learning rate at the 80000th, 100000th, 120000th 'iteration', i.e. model update or batch\n",
    "        # The paper uses a batch size of 32, which means there were about 517 iterations in an epoch\n",
    "        # Therefore, to find the epochs to decay at, you could do,\n",
    "        # if epoch in {80000 // 517, 100000 // 517, 120000 // 517}:\n",
    "        #     adjust_learning_rate(optimizer, 0.1)\n",
    "\n",
    "        # In practice, I just decayed the learning rate when loss stopped improving for long periods,\n",
    "        # and I would resume from the last best checkpoint with the new learning rate,\n",
    "        # since there's no point in resuming at the most recent and significantly worse checkpoint.\n",
    "        # So, when you're ready to decay the learning rate, just set checkpoint = 'BEST_checkpoint_ssd300.pth.tar' above\n",
    "        # and have adjust_learning_rate(optimizer, 0.1) BEFORE this 'for' loop\n",
    "\n",
    "        # One epoch's training\n",
    "        loss_train = train(train_loader=train_loader,\n",
    "              model=model,\n",
    "              criterion=criterion,\n",
    "              optimizer=optimizer,\n",
    "              epoch=epoch)\n",
    "\n",
    "        # One epoch's validation\n",
    "        val_loss = validate(val_loader=val_loader,\n",
    "                            model=model,\n",
    "                            criterion=criterion)\n",
    "\n",
    "        # plot the last values\n",
    "        plotter.add_values(epoch,loss_train=loss_train,\n",
    "                               loss_val=val_loss,redraw = False)\n",
    "\n",
    "        # As the plot is non-blocking, we should call plotter.block() at the end, to\n",
    "        # change it to the blocking-mode. Otherwise the program would instantly end\n",
    "        # and thereby close the plot.\n",
    "        # Did validation loss improve?\n",
    "        is_best = val_loss < best_loss\n",
    "        best_loss = min(val_loss, best_loss)\n",
    "\n",
    "        if not is_best:\n",
    "            epochs_since_improvement += 1\n",
    "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
    "\n",
    "        else:\n",
    "            epochs_since_improvement = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(epoch, epochs_since_improvement, model, optimizer, val_loss, best_loss, is_best)\n",
    "\n",
    "    plotter.redraw()\n",
    "    plotter.block()\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    One epoch's training.\n",
    "    :param train_loader: DataLoader for training data\n",
    "    :param model: model\n",
    "    :param criterion: MultiBox loss\n",
    "    :param optimizer: optimizer\n",
    "    :param epoch: epoch number\n",
    "    \"\"\"\n",
    "    model.train()  # training mode enables dropout\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses = AverageMeter()  # loss\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Batches\n",
    "    for i, (images, boxes, labels, _) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        # Move to default device\n",
    "        images = images.to(device)  # (batch_size (N), 3, 300, 300)\n",
    "\n",
    "        boxes = [b.to(device) for b in boxes]\n",
    "        labels = [l.to(device) for l in labels]\n",
    "\n",
    "        # Forward prop.\n",
    "        predicted_locs, predicted_scores = model(images)  # (N, 8732, 4), (N, 8732, n_classes)\n",
    "        # Loss\n",
    "        loss = criterion(predicted_locs, predicted_scores, boxes, labels)  # scalar\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        # Backward prop.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients, if necessary\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer, grad_clip)\n",
    "\n",
    "        # Update model\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(train_loader),\n",
    "                                                                  batch_time=batch_time,\n",
    "                                                                  data_time=data_time, loss=losses))\n",
    "    return(losses.avg)\n",
    "    del predicted_locs, predicted_scores, images, boxes, labels  # free some memory since their histories may be stored\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    \"\"\"\n",
    "    One epoch's validation.\n",
    "    :param val_loader: DataLoader for validation data\n",
    "    :param model: model\n",
    "    :param criterion: MultiBox loss\n",
    "    :return: average validation loss\n",
    "    \"\"\"\n",
    "    model.eval()  # eval mode disables dropout\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Prohibit gradient computation explicity because I had some problems with memory\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i, (images, boxes, labels, difficulties) in enumerate(val_loader):\n",
    "\n",
    "            # Move to default device\n",
    "            images = images.to(device)  # (N, 3, 300, 300)\n",
    "            boxes = [b.to(device) for b in boxes]\n",
    "            labels = [l.to(device) for l in labels]\n",
    "\n",
    "            # Forward prop.\n",
    "            predicted_locs, predicted_scores = model(images)  # (N, 8732, 4), (N, 8732, n_classes)\n",
    "\n",
    "            # Loss\n",
    "            loss = criterion(predicted_locs, predicted_scores, boxes, labels)\n",
    "\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            batch_time.update(time.time() - start)\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            # Print status\n",
    "            if i % print_freq == 0:\n",
    "                print('[{0}/{1}]\\t'\n",
    "                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(i, len(val_loader),\n",
    "                                                                      batch_time=batch_time,\n",
    "                                                                      loss=losses))\n",
    "\n",
    "    print('\\n * LOSS - {loss.avg:.3f}\\n'.format(loss=losses))\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
